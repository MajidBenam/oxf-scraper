{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "listed-casino",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "addressed-desktop",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-54f7268a80b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmy_main_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_func_smaller_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# get the end time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0met\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# get the execution time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "my_main_dic = test_func_smaller_soup(my_list)\n",
    "# get the end time\n",
    "et = time.time()\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(my_main_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-tokyo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "with open('my_json_file_of_oxford_test_dic.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in my_sngs:\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dic = test_func(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(my_dic[\"wise\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-administrator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-airplane",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"maroon\"\n",
    "word_dic = {}\n",
    "entry_dic = {}\n",
    "for index, item in enumerate(my_main_defs):\n",
    "    inner_dic = {}\n",
    "    for i in range(len(item)):\n",
    "        a_def = item[i].text\n",
    "        good_key_for_def = \"DEF_\" + str(i+1)\n",
    "        inner_dic[good_key_for_def] = a_def\n",
    "        print(a_def)\n",
    "    entry_dic[index] = inner_dic\n",
    "word_dic[word] = entry_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(word_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-logistics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-renaissance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-cleaning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-technical",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-perspective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-production",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree, html\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import bs4\n",
    "\n",
    "\n",
    "source_url = 'https://www.oxfordlearnersdictionaries.com/definition/english/'\n",
    "\n",
    "# cool trick\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "}\n",
    "\n",
    "my_list = [\"cushion\", \"trivial\", \"wanderer\"]\n",
    "my_words_dic = {}\n",
    "#source = requests.get(source_url, headers=headers).text\n",
    "for word in my_list:\n",
    "    my_dic = {}\n",
    "    try:\n",
    "        my_word_url = source_url + word\n",
    "        source = requests.get(my_word_url, headers=headers)\n",
    "        soup = BeautifulSoup(source.content.decode('utf-8'), 'lxml')\n",
    "        main_defs = soup.find_all('span', class_ = 'def')\n",
    "    except:\n",
    "        print(\"Bad Word...\")\n",
    "        \n",
    "    my_li = soup.find(id=\"cushion_sng_1\")\n",
    "    # selects the main text\n",
    "\n",
    "    main_examples = soup.find_all('span', class_ = 'x')\n",
    "    potential_image_links = soup.find_all('a', href=True)\n",
    "    #potential_image_links = soup.find_all('img')\n",
    "\n",
    "    # https://www.oxfordlearnersdictionaries.com/media/english/fullsize\n",
    "\n",
    "    for index, my_def in enumerate(main_defs):\n",
    "        my_dic[\"definition_\" + str(index+1)] = my_def.text\n",
    "        print(my_def.text)\n",
    "\n",
    "    for jndex, my_ex in enumerate(main_examples):\n",
    "        my_dic[\"example_set_\" + str(jndex+1)] = my_ex.text\n",
    "        print(my_ex.text)\n",
    "\n",
    "    for my_pic in potential_image_links:\n",
    "        if \"https://www.oxfordlearnersdictionaries.com/media/english/fullsize\" in my_pic[\"href\"]:\n",
    "            print(my_pic['href'])\n",
    "\n",
    "    my_words_dic[word] = my_dic\n",
    "    #print(len(main_defs))\n",
    "    #print(len(main_examples))\n",
    "    #print(len(potential_image_links))\n",
    "    print(my_li)\n",
    "\n",
    "for k, v in my_words_dic.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "\n",
    "#with open('ref_seshat.csv', 'w') as my_file:\n",
    "#    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree, html\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import bs4\n",
    "\n",
    "\n",
    "source_url = 'https://www.oxfordlearnersdictionaries.com/definition/english/'\n",
    "\n",
    "# cool trick\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "}\n",
    "\n",
    "my_list = [\"wanderlust\", \"cushion\", \"wanderer\"]\n",
    "my_words_list_of_dics = []\n",
    "#source = requests.get(source_url, headers=headers).text\n",
    "for word in my_list:\n",
    "    my_dic_for_this_word = {\n",
    "        \"word\": word,\n",
    "        \"pos\": \"xyz\",\n",
    "        \"phon\": \"abc\",\n",
    "    }\n",
    "    for i in [1,2,3,4]:\n",
    "        my_word_url = source_url + word + \"_\" + str(i)\n",
    "        source = requests.get(my_word_url, headers=headers)\n",
    "        soup = BeautifulSoup(source.content.decode('utf-8'), 'lxml')\n",
    "        main_defs = soup.find_all('span', class_ = 'def')\n",
    "        \n",
    "        if not main_defs:\n",
    "            continue\n",
    "        \n",
    "        # parts of speech\n",
    "        try:\n",
    "            my_pos = soup.find('span', class_ = 'pos')\n",
    "            my_dic_for_this_word[\"pos\"] = my_pos.text\n",
    "        except:\n",
    "            my_dic_for_this_word[\"pos\"] = \"NoPOSFound\"\n",
    "\n",
    "        # phonetics\n",
    "        try:\n",
    "            my_american_pros = soup.find('div', class_=\"phons_n_am\")\n",
    "            my_american_pro = my_american_pros.find_all('span', class_ = 'phon')\n",
    "            my_dic_for_this_word[\"phon\"] = my_american_pro[0].text\n",
    "        except:\n",
    "            my_dic_for_this_word[\"phon\"] = \"NoProFound\"\n",
    "            \n",
    "        # ol with a class of sense_single or senses_multiple\n",
    "        try:\n",
    "            my_multiple_defs = soup.find('ol', class_=\"senses_multiple\")\n",
    "            # number of defs\n",
    "            main_defs = my_multiple_defs.find('span', class_ = 'def')\n",
    "            for index, my_def in enumerate(main_defs):\n",
    "                list_item_id = word + \"_sng_\" + str(index+1)\n",
    "                my_smaller_soup = my_def.find(id=list_item_id)\n",
    "                my_dic_for_this_word[\"def_\" + str(index+1)] = my_def.text\n",
    "                try:\n",
    "                    my_examples = my_smaller_soup.find_all('span', class_ = 'x')\n",
    "                    my_dic[\"example_set_\" + str(index+1)] = [my_ex.text for my_ex in my_examples]\n",
    "                except:\n",
    "                    my_dic[\"example_set_\" + str(index+1)] = \"NoExamples\"              \n",
    "        except:\n",
    "            my_single_def = soup.find('ol', class_=\"sense_single\")\n",
    "            list_item_id = word + \"_sng_1\"\n",
    "            my_smaller_soup = my_single_def.find(id=list_item_id)\n",
    "            # the only def:\n",
    "            main_def = my_smaller_soup.find('span', class_ = 'def')\n",
    "            my_dic_for_this_word[\"def_1\"] = main_def.text\n",
    "            # the only set of examples:\n",
    "            try:\n",
    "                main_examples = my_smaller_soup.find_all('span', class_ = 'x')\n",
    "                my_dic[\"example_set_1\"] = [my_ex.text for my_ex in main_examples]\n",
    "            except:\n",
    "                my_dic[\"example_set_1\"] = \"NoExamples\"\n",
    "        my_words_list_of_dics.append(my_dic_for_this_word)\n",
    "        \n",
    "\n",
    "\n",
    "        main_examples = my_smaller_soup.find_all('span', class_ = 'x')\n",
    "        \n",
    "        \n",
    "        # images\n",
    "        potential_image_links = soup.find_all('a', href=True)\n",
    "        #potential_image_links = soup.find_all('img')\n",
    "\n",
    "        for jndex, my_ex in enumerate(main_examples):\n",
    "            my_dic[\"example_set_\" + str(jndex+1)] = my_ex.text\n",
    "            print(my_ex.text)\n",
    "        print(my_american_pro[0].text)\n",
    "\n",
    "\n",
    "#with open('ref_seshat.csv', 'w') as my_file:\n",
    "#    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cushion_topg_3 > div > span.phonetics > div.phons_n_am > span\n",
    "#wanderer_topg_1 > div > span.phonetics > div.phons_n_am > span\n",
    "#wanderlust_topg_1 > div > span.phonetics > div.phons_n_am > span\n",
    "#take_topg_2 > div > span.phonetics > div.phons_br > span\n",
    "#take_topg_2 > div > span.phonetics > div.phons_n_am > span\n",
    "\n",
    "#take_topg_57 > div > span.phonetics > div.phons_n_am > span\n",
    "\n",
    "#wanderlust_topg_1 > div > span.phonetics > div.phons_n_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-roller",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pet does not work properly because pet_sng_4 is the first id ... Weird\n",
    "\n",
    "my_words_list_of_dics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_converter = {\n",
    "    \"noun\": \"n\",\n",
    "    \"verb\": \"v\",\n",
    "    \"adjective\": \"adj\",\n",
    "    \"adverb\": \"adv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pos_converter.get(\"tr\", \"noun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \" abc def fdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.lstrip().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
